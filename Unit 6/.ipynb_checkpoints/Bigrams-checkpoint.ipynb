{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been shown that the inclusion of word bigram features leads to imporved sentiment analysis for various classifiers (Wang and Manning, 2012).\n",
    "In this example, we will see whether the use of bigrams as features improve the accuracy of our sentiment classification. \n",
    "\n",
    "References:\n",
    "Wang and Manning. Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. In ACL, 2012. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import these tools\n",
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3:\n",
    "    \n",
    "Here, you will need to create a function called \"bigrams_as_features\" which returns the 200 most frequent bigrams. \n",
    "\n",
    "One way to do so is to find significant bigrams by using nltk.collocations.BigramCollocationFinder along with nltk.metrics.BigramAssocMeasures. \n",
    "\n",
    "From NLTK: Finding collocations requires first calculating the frequencies of words and\n",
    "their appearance in the context of other words. Often the collection of words\n",
    "will then requiring filtering to only retain useful content terms. Each ngram\n",
    "of words may then be scored according to some association measure, in order\n",
    "to determine the relative likelihood of each ngram being a collocation.\n",
    "\n",
    "The ``BigramCollocationFinder`` and ``TrigramCollocationFinder`` classes provide\n",
    "these functionalities, dependent on being provided a function which scores a\n",
    "ngram given appropriate frequency counts. A number of standard association\n",
    "measures are provided in bigram_measures and trigram_measures.\n",
    "\n",
    "For more details and a demo see here: http://www.nltk.org/_modules/nltk/collocations.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_as_features(words, score_bg=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_bg, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, this is just the same code as previously. The only thing that changes is the feature extraction method, \n",
    "#which is user specified.\n",
    "def evaluate_classifier(feature_extraction):\n",
    "    \n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    " \n",
    "    negreviews = [(feature_extraction(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "    posreviews = [(feature_extraction(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "    negsplit = int(len(negreviews)*0.75)\n",
    "    possplit = int(len(posreviews)*0.75)\n",
    "\n",
    "    trainingset = negreviews[:negsplit] + posreviews[:possplit]\n",
    "    testset = negreviews[negsplit:] + posreviews[possplit:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(trainingset)\n",
    " \n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testset))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the function let's now run it as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.816\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "          ('give', 'us') = True              neg : pos    =     12.3 : 1.0\n",
      "       ('matt', 'damon') = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "    ('absolutely', 'no') = True              neg : pos    =     10.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(bigrams_as_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "Is the accuracy improved? What do you think it would happen if we had used all bigrams rather than the top-200? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
